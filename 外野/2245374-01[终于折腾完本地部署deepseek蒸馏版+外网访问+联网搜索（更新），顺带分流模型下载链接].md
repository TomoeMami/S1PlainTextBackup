
*****

####  琉璃苑軒風  
##### 1#       楼主       发表于 2025-2-6 15:03

 本帖最后由 琉璃苑軒風 于 2025-2-7 13:38 编辑 

看各路教程拼拼凑凑，我在实操中感觉其他其实都没什么卡点，但是最麻烦的反倒是模型的下载，动不动速度就掉到就几MB到几百KB，

所以我既然下完了，就分流一下ollama的默认精度（8B 14B 32B）少一个被下载模型的折腾都好，拷贝到目录里就行，这个反正跑前都要核验过MD5的不用担心

32B（能满足比较基础的要求，需要22-24显存，除了基础占用还有开始输入输出的额外占用，不能卡太死，可多卡凑一起，但是会慢点）

链接：[https://pan.baidu.com/s/1SvweXjREmcxplOkqj-uBkw?pwd=DEEP](https://pan.baidu.com/s/1SvweXjREmcxplOkqj-uBkw?pwd=DEEP) 

提取码：DEEP

14B（大概能有点用，需要10-12g显存）

链接：[https://pan.baidu.com/s/1CUUvIlmWMbqy2tW7TxJZDQ?pwd=DEEP](https://pan.baidu.com/s/1CUUvIlmWMbqy2tW7TxJZDQ?pwd=DEEP) 

提取码：DEEP

8B（8g显存左右的卡都能跑，能回话，其他不要想太多）

链接：[https://pan.baidu.com/s/1KM0-p_XXafeZiETeidxfxA?pwd=DEEP](https://pan.baidu.com/s/1KM0-p_XXafeZiETeidxfxA?pwd=DEEP) 

提取码：DEEP

有其他问题要问我能回答我都回答，折腾了一会终于有点明白了，还躺了不少雷

如果能帮忙传个硅基流动的火也谢谢你，毕竟蒸馏就是服务器过载时候能用，肯定比不上完整部署的671B
[https://cloud.siliconflow.cn/i/erfbHC4B](https://cloud.siliconflow.cn/i/erfbHC4B)

联网可以尝试用Page Assist插件（需要加速器）

chrome商店搜索Page Assist，也是类似chatboxai配置的傻瓜式操作 参考[https://www.bilibili.com/opus/1030404105077522451](https://www.bilibili.com/opus/1030404105077522451)

实际效果举例

<img src="https://img.saraba1st.com/forum/202502/07/133709v2c6hmmp4494q444.jpg" referrerpolicy="no-referrer">

<strong>WechatIMG693.jpg</strong> (53.18 KB, 下载次数: 0)

下载附件

2025-2-7 13:37 上传

<img src="https://img.saraba1st.com/forum/202502/07/133710g0r028drzgdhy9lc.jpg" referrerpolicy="no-referrer">

<strong>WechatIMG694.jpg</strong> (133.82 KB, 下载次数: 0)

下载附件

2025-2-7 13:37 上传

<img src="https://img.saraba1st.com/forum/202502/06/145455dbtqttxcdgc11g01.jpg" referrerpolicy="no-referrer">

<strong>WechatIMG683.jpg</strong> (106.07 KB, 下载次数: 2)

下载附件

2025-2-6 14:54 上传

<img src="https://img.saraba1st.com/forum/202502/06/145455lyv2xabnu8xy7pu7.jpg" referrerpolicy="no-referrer">

<strong>WechatIMG684.jpg</strong> (178.53 KB, 下载次数: 1)

下载附件

2025-2-6 14:54 上传

﹍﹍﹍

评分

 参与人数 3战斗力 +4

|昵称|战斗力|理由|
|----|---|---|
| 429| + 2|好评加鹅|
| Horla| + 1|好评加鹅|
| a4ac7| + 1|好评加鹅|

查看全部评分

*****

####  我被骗了五块钱  
##### 2#       发表于 2025-2-6 15:04

显存不够可以用内存凑吗<img src="https://static.saraba1st.com/image/smiley/face/151.gif" referrerpolicy="no-referrer">

*****

####  琉璃苑軒風  
##### 3#         楼主| 发表于 2025-2-6 15:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>

显存不够可以用内存凑吗</blockquote>
可以，但是非常非常慢，70B我跑起来first token latency等到我没耐心了

我是四通道D4，可能12通道D5的epyc能好一些

*****

####  黄泉川此方  
##### 4#       发表于 2025-2-6 15:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>

显存不够可以用内存凑吗</blockquote>
可以，我试过24显存+32内存跑70B

很卡，也没比32B强很多

*****

####  天地一机成化育  
##### 5#       发表于 2025-2-6 15:14

目前对普通人来说本地部署依旧看不到性价比

*****

####  袄_偶滴小乔  
##### 6#       发表于 2025-2-6 15:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>
显存不够可以用内存凑吗</blockquote>
显存不够的时候gpu好像不工作的，是cpu在跑。

我68xt跑14b，全用显存的速度大概30token/s，跑更大模型16g显存不够用，走cpu运算直接崩到2token/s

*****

####  claymorep  
##### 7#       发表于 2025-2-6 15:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358620&amp;ptid=2245374" target="_blank">袄_偶滴小乔 发表于 2025-2-6 15:14</a>
显存不够的时候gpu好像不工作的，是cpu在跑。

我68xt跑14b，全用显存的速度大概30token/s，跑更大模型16 ...</blockquote>
好吧，我也16g显存，看来不能用更大的<img src="https://static.saraba1st.com/image/smiley/face2017/007.png" referrerpolicy="no-referrer">

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  琉璃苑軒風  
##### 8#         楼主| 发表于 2025-2-6 15:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358666&amp;ptid=2245374" target="_blank">claymorep 发表于 2025-2-6 15:19</a>

好吧，我也16g显存，看来不能用更大的

—— 来自 鹅球 v3.3.96-alpha</blockquote>
意外之喜是可以多卡，我all in booom上本来想把8g那张卡卖掉的，结果误打误撞可以当24g的卡用了

*****

####  琉璃苑軒風  
##### 9#         楼主| 发表于 2025-2-6 15:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358619&amp;ptid=2245374" target="_blank">天地一机成化育 发表于 2025-2-6 15:14</a>

目前对普通人来说本地部署依旧看不到性价比</blockquote>
等迭代吧，我看评分，现在ds R1蒸馏出来的14B都比早先的的32B甚至70B强了

*****

####  makourisu-2  
##### 10#       发表于 2025-2-6 15:49

请问4070的笔记本能不能跑起来8B或者再小一点的模型<img src="https://static.saraba1st.com/image/smiley/face2017/007.png" referrerpolicy="no-referrer">有点好奇，想折腾玩玩看

*****

####  琉璃苑軒風  
##### 11#         楼主| 发表于 2025-2-6 15:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358918&amp;ptid=2245374" target="_blank">makourisu-2 发表于 2025-2-6 15:49</a>

请问4070的笔记本能不能跑起来8B或者再小一点的模型有点好奇，想折腾玩玩看 ...</blockquote>
可以8B以及以下，就是效果真就图一乐了

*****

####  tsukicn  
##### 12#       发表于 2025-2-6 15:56

有没有70b的，直接下太慢了。。。

*****

####  sese199  
##### 13#       发表于 2025-2-6 15:59

我看nga上一个帖子，说8g显存+32g内存就能跑32b q4的蒸馏模型？

*****

####  子虚乌有  
##### 14#       发表于 2025-2-6 16:00

本地跑可以写刘备文了吗

*****

####  moekyo  
##### 15#       发表于 2025-2-6 16:12

国内不是有这个对标的分发网站吗
[https://modelscope.cn/models](https://modelscope.cn/models)

*****

####  xburke  
##### 16#       发表于 2025-2-6 16:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359003&amp;ptid=2245374" target="_blank">sese199 发表于 2025-2-6 15:59</a>

我看nga上一个帖子，说8g显存+32g内存就能跑32b q4的蒸馏模型？</blockquote>
4g显存+32g内存就可以

*****

####  黄泉川此方  
##### 17#       发表于 2025-2-6 16:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359011&amp;ptid=2245374" target="_blank">子虚乌有 发表于 2025-2-6 16:00</a>

本地跑可以写刘备文了吗</blockquote>
写出来的是地摊文学级别

70B消融以后会变成弱智

*****

####  琉璃苑軒風  
##### 18#         楼主| 发表于 2025-2-6 16:20

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359003&amp;ptid=2245374" target="_blank">sese199 发表于 2025-2-6 15:59</a>

我看nga上一个帖子，说8g显存+32g内存就能跑32b q4的蒸馏模型？</blockquote>
速度正常么？我四通道D4反正拉稀的一塌糊涂。

*****

####  琉璃苑軒風  
##### 19#         楼主| 发表于 2025-2-6 16:21

 本帖最后由 琉璃苑軒風 于 2025-2-6 16:26 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359011&amp;ptid=2245374" target="_blank">子虚乌有 发表于 2025-2-6 16:00</a>

本地跑可以写刘备文了吗</blockquote>
没试过，看攻略是破防容易，但是本地的精度，写出来很是不好看

*****

####  琉璃苑軒風  
##### 20#         楼主| 发表于 2025-2-6 16:24

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359112&amp;ptid=2245374" target="_blank">moekyo 发表于 2025-2-6 16:12</a>

国内不是有这个对标的分发网站吗

https://modelscope.cn/models</blockquote>
没有合适的网络，这个也不快

*****

####  moekyo  
##### 21#       发表于 2025-2-6 16:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359218&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-6 16:24</a>

没有合适的网络，这个也不快</blockquote>
我又搜到了这两个，当然我也没试过就是了
[https://hf-mirror.com/](https://hf-mirror.com/)
[https://aifasthub.com/](https://aifasthub.com/)

*****

####  chaosliu  
##### 22#       发表于 2025-2-6 16:35

我用4070tis也能部署32B的蒸馏模型，token的生成速度也可以接受<img src="https://static.saraba1st.com/image/smiley/face2017/018.png" referrerpolicy="no-referrer">

*****

####  xing7673  
##### 23#       发表于 2025-2-6 16:37

我还以为你折腾了deepseek的联网搜索功能

正需要这个东西

*****

####  王苍幻  
##### 24#       发表于 2025-2-6 16:43

但是个人使用本地部署真的没啥意义
api就足够了

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  王苍幻  
##### 25#       发表于 2025-2-6 16:44

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359297&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-6 16:35</a>
我用4070tis也能部署32B的蒸馏模型，token的生成速度也可以接受</blockquote>
同显卡
那速度我不能忍受

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  zhao25  
##### 26#       发表于 2025-2-6 16:50

模型下好了，是直接覆盖到我之前的文件夹里面吗

*****

####  海底铁锚  
##### 27#       发表于 2025-2-6 16:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359316&amp;ptid=2245374" target="_blank">xing7673 发表于 2025-2-6 16:37</a>
我还以为你折腾了deepseek的联网搜索功能

正需要这个东西</blockquote>
用anything llm就可以了，有比较基础的搜索。

*****

####  kira1988  
##### 28#       发表于 2025-2-6 16:50

要本地部署满血版需要什么配置

*****

####  大十字紅朔  
##### 29#       发表于 2025-2-6 16:50

可以写涩涩吗？

*****

####  琉璃苑軒風  
##### 30#         楼主| 发表于 2025-2-6 16:55

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359426&amp;ptid=2245374" target="_blank">kira1988 发表于 2025-2-6 16:50</a>

要本地部署满血版需要什么配置</blockquote>
1342g基础显存+额外输入输出显存，别说个人，中小企业都很难有这个配置

*****

####  琉璃苑軒風  
##### 31#         楼主| 发表于 2025-2-6 16:56

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359423&amp;ptid=2245374" target="_blank">zhao25 发表于 2025-2-6 16:50</a>

模型下好了，是直接覆盖到我之前的文件夹里面吗</blockquote>
是的

。zsbd

*****

####  琉璃苑軒風  
##### 32#         楼主| 发表于 2025-2-6 19:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359369&amp;ptid=2245374" target="_blank">王苍幻 发表于 2025-2-6 16:43</a>

但是个人使用本地部署真的没啥意义

api就足够了</blockquote>
我主楼已经有提到了，“毕竟蒸馏就是服务器过载时候能用，肯定比不上完整部署的671B”

api能正常用不抽风，那肯定比本地好阿，但是现在这个ds日常抽风，硅基间歇抽风的前提下

本地部署是决定了下限，而非上限

*****

####  精钢魔像  
##### 33#       发表于 2025-2-6 19:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359425&amp;ptid=2245374" target="_blank">海底铁锚 发表于 2025-2-6 16:50</a>

用anything llm就可以了，有比较基础的搜索。</blockquote>
效果怎么样，能有kimi的水平吗

*****

####  Lsky  
##### 34#       发表于 2025-2-6 20:47

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359426&amp;ptid=2245374" target="_blank">kira1988 发表于 2025-2-6 16:50</a>

要本地部署满血版需要什么配置</blockquote>
[https://www.bilibili.com/video/BV1REPqeFE6d/](https://www.bilibili.com/video/BV1REPqeFE6d/)

从这个视频看的话，4B量化版本，用cpu跑，用400g左右的内存可以

成本50000

*****

####  FeteFete  
##### 35#       发表于 2025-2-6 20:49

量化了么？

要是量化的话用的是多少量化?

*****

####  肥胖的道奇兔  
##### 36#       发表于 2025-2-6 21:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>

显存不够可以用内存凑吗</blockquote>
用16G显存的卡能跑32b吗

*****

####  darktide  
##### 37#       发表于 2025-2-6 21:32

本地部署有啥教程没？包括联网搜索的

*****

####  琉璃苑軒風  
##### 38#         楼主| 发表于 2025-2-6 22:10

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67361219&amp;ptid=2245374" target="_blank">darktide 发表于 2025-2-6 21:32</a>

本地部署有啥教程没？包括联网搜索的</blockquote>
这个一大堆，我就是东拼西凑发现没有卡点，除了下模型下的要死要活，所以做了下分流

﹍﹍﹍

评分

 参与人数 1战斗力 +2

|昵称|战斗力|理由|
|----|---|---|
| darktide| + 2|好的，谢谢|

查看全部评分

*****

####  琉璃苑軒風  
##### 39#         楼主| 发表于 2025-2-6 22:52

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359316&amp;ptid=2245374" target="_blank">xing7673 发表于 2025-2-6 16:37</a>

我还以为你折腾了deepseek的联网搜索功能

正需要这个东西</blockquote>
咦？刚才没回复上？

这个挂Page Assist就行（搜这个名字就有一大堆教程），比是比不上原版，但是也有明显改善结果

*****

####  シマエナガ  
##### 40#       发表于 2025-2-7 08:20

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358918&amp;ptid=2245374" target="_blank">makourisu-2 发表于 2025-2-6 15:49</a>

请问4070的笔记本能不能跑起来8B或者再小一点的模型有点好奇，想折腾玩玩看 ...</blockquote>
内存够大就行 跑GGUF可以


*****

####  zhao25  
##### 41#       发表于 2025-2-7 08:58

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67361140&amp;ptid=2245374" target="_blank">肥胖的道奇兔 发表于 2025-2-6 21:19</a>

用16G显存的卡能跑32b吗</blockquote>
我12G显存，跑起来了32b的，就是有点慢，估计是用内存了。。。

*****

####  mandown  
##### 42#       发表于 2025-2-7 10:00

太厉害了，收藏

*****

####  xibeijian  
##### 43#       发表于 2025-2-7 10:24

 本帖最后由 xibeijian 于 2025-2-8 18:00 编辑 

本地跑模型的UI很多，比如 lm studio，ollama，前者在 huggingface 上有专门的space 提供 gguf 量化模型， 后者也有自己的一个网址（[https://ollama.com/](https://ollama.com/)）提供ui 和 模型的下载，我本人主要使用 lm studio，模型下载地址是 hf-mirror（huggingface的非官方路由），macos 使用方法如下：

新版的 lm studio 提供了代理方式，步骤 2 之后全部省掉。

1. 首先下载 lm-studio（[https://lmstudio.ai/](https://lmstudio.ai/)）

2. 登陆 [https://huggingface.co/](https://huggingface.co/) （需要代理）并注册，用户名为参数3，并生成一个 token，作为参数4

3. 下载 hfd ([https://gist.github.com/padeoe/697678ab8e528b85a2a7bddafea1fa4f](https://gist.github.com/padeoe/697678ab8e528b85a2a7bddafea1fa4f))  或者去 （[https://hf-mirror.com/](https://hf-mirror.com/)）手动下载。

4. 进入到 ～/.cache/lm-studio/models/Publisher/Repository 目录下（没有就手动创建），将 下载的 hfd 放到此目录下。

5. 选择一个量化模型，比如 [https://hf-mirror.com/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF](https://hf-mirror.com/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF) 

5.1 参数1: unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF ，就是

<img src="https://img.saraba1st.com/forum/202502/07/101742guhcr1ra91wvtq4p.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (9.41 KB, 下载次数: 0)

下载附件

2025-2-7 10:17 上传

这里点下复制

5.2 参数2:DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf

<img src="https://img.saraba1st.com/forum/202502/07/101821gm5qignltdjnonyo.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (76.43 KB, 下载次数: 0)

下载附件

2025-2-7 10:18 上传

这里选中，根据自己的配置选一个，建议 Q6 以上

6. 利用 hdf 命令下载模型

6.1 export HF_ENDPOINT=https://hf-mirror.com ：替换 huggingface的下载地址

6.2 下载模板：./hfd.sh 参数1 --include 参数2 --tool aria2c -x 4 --hf_username 参数3 --hf_token 参数4

6.3 例子中的下载请求就是：./hfd.sh unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF --include DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf--tool aria2c -x 4 --hf_username 参数3 --hf_token 参数4

7 打开 lm studio ，加载模型即可

<img src="https://img.saraba1st.com/forum/202502/07/102421mc7zn24to4oceipw.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (39.92 KB, 下载次数: 0)

下载附件

2025-2-7 10:24 上传

注意：

windows 上如果使用 WLS 的话，其实整体操作和macos上没任何区别，只是整个操作要在 wls 下，而不是windows下执行，模型下载完毕后放到 windows 目录下的～/.cache/lm-studio/models/Publisher/Repository 即可。

mac m1p 16G，目前测试最多可以加载 14B模型的Q6量化模型，但加载完，其他应用也不必加载了，所以我试下来合理的模型大小是 8B-9B，Q6/8量化模型。需要明确的是，本地使用大模型更多关注的是他的推理能力，API调用能力，以及代码生成能力（很适合开发），如果是用来做文档等内容工作，建议用 SaaS版的API。在这种需求下，除了使用APP和线上的 chat/agent ，本地的 UI 可选也不少，比如：

1. dify

2. anything llm

3. chatbox

*****

####  琉璃苑軒風  
##### 44#         楼主| 发表于 2025-2-7 10:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67365950&amp;ptid=2245374" target="_blank">xibeijian 发表于 2025-2-7 10:24</a>

本地跑模型的UI很多，比如 lm studio，ollama，前者在 huggingface 上有专门的space 提供 gguf 量化模型，  ...</blockquote>
能问下这个和qwen蒸馏的版本区别是什么啊？

*****

####  xibeijian  
##### 45#       发表于 2025-2-7 10:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67365983&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-7 10:27</a>

能问下这个和qwen蒸馏的版本区别是什么啊？</blockquote>
GGUF量化只是将模型规模变小，本地较小的内存和显存下可以执行并推理，和蒸馏没关系。 蒸馏本质上其实就是嫁接，有一个优秀的底座，比如 deepseek-r1，其他的模型以此为底，将优秀的推理能力拿出来，给其他模型使用，比如deepseek-v3，比如例子中的 deepseek-r1-distill-llama（qwen） 等，你可以理解为蒸馏就是传道授业。

*****

####  琉璃苑軒風  
##### 46#         楼主| 发表于 2025-2-7 10:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67366040&amp;ptid=2245374" target="_blank">xibeijian 发表于 2025-2-7 10:32</a>

GGUF量化只是将模型规模变小，本地较小的内存和显存下可以执行并推理，和蒸馏没关系。 蒸馏本质上其实就 ...</blockquote>
饿，我不是太专业问的不太对，就是i这个llama后缀的和qwen后缀的是有什么区别啊？

就是嫁接的模型不太一样？

*****

####  xibeijian  
##### 47#       发表于 2025-2-7 10:56

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67366282&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-7 10:51</a>

饿，我不是太专业问的不太对，就是i这个llama后缀的和qwen后缀的是有什么区别啊？

就是嫁接的模型不太一 ...</blockquote>
看这个链接吧：
[https://blog.csdn.net/fuhanghang/article/details/145033277](https://blog.csdn.net/fuhanghang/article/details/145033277)

﹍﹍﹍

评分

 参与人数 1战斗力 +2

|昵称|战斗力|理由|
|----|---|---|
| 琉璃苑軒風| + 2|谢谢你|

查看全部评分

*****

####  琉璃苑軒風  
##### 48#         楼主| 发表于 2025-2-7 13:38

更新下联网展示

*****

####  aithinkso  
##### 49#       发表于 2025-2-7 13:51

Ollama我一直是直连下载没有问题
LMStudio最新版本提供了hf代理选项，搜索下载模型简单多了

—— 来自 [鹅球](https://www.pgyer.com/GcUxKd4w) v3.3.96

*****

####  xing7673  
##### 50#       发表于 2025-2-7 15:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67362519&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-6 22:52</a>

咦？刚才没回复上？

这个挂Page Assist就行（搜这个名字就有一大堆教程），比是比不上原版，但是也有明 ...</blockquote>
嗯，谢谢，不过刚才有位大佬推荐的anythingLLM这个更全面一些，我还在研究。

*****

####  tsukicn  
##### 51#       发表于 2025-2-8 16:01

我有4张2080ti11g，为什么用ollama跑32b的ds时，4张显存占用倒是差不多，但是只有1张显卡的利用率上去了，别的3张都没动，速度也就7 tokens/s上下。。。

*****

####  琉璃苑軒風  
##### 52#         楼主| 发表于 2025-2-8 17:46

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67375592&amp;ptid=2245374" target="_blank">tsukicn 发表于 2025-2-8 16:01</a>

我有4张2080ti11g，为什么用ollama跑32b的ds时，4张显存占用倒是差不多，但是只有1张显卡的利用率上去了， ...</blockquote>
因为是显存共享，只看一张卡。。。

*****

####  chaosliu  
##### 53#       发表于 2025-2-8 17:47

想问下lz是部署在虚拟机上还是直接部署的？我下载了ollama安装后，直接输入指令把蒸馏模型下载下来，但不知道怎么把模型弄到其他盘里，以及安装什么图形界面，现在只能CMD界面一个个敲来对话好麻烦<img src="https://static.saraba1st.com/image/smiley/face2017/018.png" referrerpolicy="no-referrer">

*****

####  tsukicn  
##### 54#       发表于 2025-2-8 20:05

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67376342&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-8 17:47</a>

想问下lz是部署在虚拟机上还是直接部署的？我下载了ollama安装后，直接输入指令把蒸馏模型下载下来，但不知 ...</blockquote>
移动别的盘网上有很多教程，基本就改下环境变量，再把模型考到新的盘里，不用重新下。

图形界面装个openwebui就行

*****

####  tsukicn  
##### 55#       发表于 2025-2-8 20:07

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67376336&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-8 17:46</a>

因为是显存共享，只看一张卡。。。</blockquote>
这样子吗，我以为利用率跑满才是正确的。那我速度这么慢是啥原因呢，我看别人2080ti(魔改22g)32b的有20多tokens/s欸。。

*****

####  琉璃苑軒風  
##### 56#         楼主| 发表于 2025-2-8 22:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67376342&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-8 17:47</a>

想问下lz是部署在虚拟机上还是直接部署的？我下载了ollama安装后，直接输入指令把蒸馏模型下载下来，但不知 ...</blockquote>
直接部署，

你调整模型位置就用环境变量改，主楼那个B站链接里就涉及了

然后你要图形界面，最简单的就是chatboxai，如果用openwebui在安装过程中需要加速器，不然可能会很慢

*****

####  琉璃苑軒風  
##### 57#         楼主| 发表于 2025-2-8 22:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67377047&amp;ptid=2245374" target="_blank">tsukicn 发表于 2025-2-8 20:07</a>

这样子吗，我以为利用率跑满才是正确的。那我速度这么慢是啥原因呢，我看别人2080ti(魔改22g)32b的有20多 ...</blockquote>
多卡导致的

2080ti22G勉强可以单卡32B

﹍﹍﹍

评分

 参与人数 1战斗力 +2

|昵称|战斗力|理由|
|----|---|---|
| tsukicn| + 2|好评加鹅|

查看全部评分

*****

####  chaosliu  
##### 58#       发表于 2025-2-8 22:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67377740&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-8 22:15</a>

直接部署，

你调整模型位置就用环境变量改，主楼那个B站链接里就涉及了</blockquote>
我看也有推荐fastgpt，这两个孰优孰劣？

*****

####  chaosliu  
##### 59#       发表于 2025-2-8 23:06

实际32B模型跑起来看了眼任务管理器，gpu只吃了24%，显存吃了15.2g，内存吃6g，cpu跑了52%<img src="https://static.saraba1st.com/image/smiley/face2017/001.png" referrerpolicy="no-referrer">这就是显存不够调用了内存所以token生成慢吗？

*****

####  卡修_Kasio  
##### 60#       发表于 2025-2-9 07:42

我用ollama跑了个14b的版本,发现显存和内存情况压根就没啥变动.不知道为啥,我再部署个32b版本的试试

我的机器是i7-7700,1080ti11g显存,64g内存

*****

####  琉璃苑軒風  
##### 61#         楼主| 发表于 2025-2-9 08:24

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67378074&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-8 23:06</a>
实际32B模型跑起来看了眼任务管理器，gpu只吃了24%，显存吃了15.2g，内存吃6g，cpu跑了52%这就是显存不够调 ...</blockquote>
是的，而且是断崖式速度下降

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  琉璃苑軒風  
##### 62#         楼主| 发表于 2025-2-9 12:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67379147&amp;ptid=2245374" target="_blank">卡修_Kasio 发表于 2025-2-9 07:42</a>

我用ollama跑了个14b的版本,发现显存和内存情况压根就没啥变动.不知道为啥,我再部署个32b版本的试试

我的 ...</blockquote>
14B占10G左右显存应该还是要的

*****

####  卡修_Kasio  
##### 63#       发表于 2025-2-9 14:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67380268&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-9 12:28</a>
14B占10G左右显存应该还是要的</blockquote>
是的，试了下1080ti的11g显存完全可以跑14b，不需要内存，而且出结构很流畅。就是如果搭ragflow的话可能就没显存给embedding用了

*****

####  欧比旺  
##### 64#       发表于 2025-2-9 16:33

内存不够，显卡才8g，勉强跑了个14B的本地，chatbox，anything llm cherrystudio和page assist都用了个一圈，发现还是官网联网慢思考是回答最好的，剩下硅基，腾讯， 火山的都部署了，感觉都差点意思但是最起码能用了，本地小于30b的到底能拿来做点什么呢

*****

####  琉璃苑軒風  
##### 65#         楼主| 发表于 2025-2-9 16:38

 本帖最后由 琉璃苑軒風 于 2025-2-9 16:44 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67381406&amp;ptid=2245374" target="_blank">欧比旺 发表于 2025-2-9 16:33</a>

内存不够，显卡才8g，勉强跑了个14B的本地，chatbox，anything llm cherrystudio和page assist都用了个一圈 ...</blockquote>
page assist+硅基api，可以当个官方青春版使用了，官方一直卡死的状态下，这个应该是当下性能最高的一档

其实还有个三方中转的api+硅基流动，但是这个有api泄露风险，就不怎么推荐别人用了

*****

####  UNICORN00  
##### 66#       发表于 2025-2-9 16:42

 本帖最后由 UNICORN00 于 2025-2-10 11:21 编辑 

6G显存笔记本，跑了下DeepSeek-R1-GGUF 1.73bit（模型158GB）

0.08 token/s <img src="https://static.saraba1st.com/image/smiley/face2017/217.gif" referrerpolicy="no-referrer">

哦用成纯cpu模式了，gpu加速，启动

0.2 token/s <img src="https://static.saraba1st.com/image/smiley/face2017/217.gif" referrerpolicy="no-referrer">

哦Open WebUI 新版能显示思考链，只是有些回答本身就没有思考过程。。

*****

####  梁皇无忌  
##### 67#       发表于 2025-2-9 17:57

所以现在16g的显卡是不是很尴尬，
想用4080或者5080试试32b的版本

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  simon兄  
##### 68#       发表于 2025-2-9 18:24

怎么将下载的这个模型放进LM啊

*****

####  精钢魔像  
##### 69#       发表于 2025-2-9 18:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67382059&amp;ptid=2245374" target="_blank">simon兄 发表于 2025-2-9 18:24</a>

怎么将下载的这个模型放进LM啊</blockquote>
先下载个ollama装上，在用户文件夹里有个.llama，把模型拷进去

按win+x进终端，输入ollama run deepseek-r1:8b（如果是8b），退出是/bye，5分钟不用释放显存

如果c盘空间不够你自己查查怎么改模型路径

*****

####  moekyo  
##### 70#       发表于 2025-2-9 18:47

 本帖最后由 moekyo 于 2025-2-9 18:50 编辑 

我自己的M1 16搭配ollama和沉浸式翻译跑qwen 14b还行，比API稳定多了，效果也勉强能接受<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">

*****

####  琉璃苑軒風  
##### 71#         楼主| 发表于 2025-2-9 18:57

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67381903&amp;ptid=2245374" target="_blank">梁皇无忌 发表于 2025-2-9 17:57</a>

所以现在16g的显卡是不是很尴尬，

想用4080或者5080试试32b的版本</blockquote>
你要台式机找一张8g老卡一起插上去，虽然慢一些但是就能跑32B了

*****

####  流缨  
##### 72#       发表于 2025-2-9 19:16

 本帖最后由 流缨 于 2025-2-9 19:25 编辑 

插件好用，搭配32b有API的6成功力了，QWEN这个基座真的不太行，没有满血版的灵气，温度1.3还是很木头

4090已经成为我近年来投资回报率最高的实物产品<img src="https://static.saraba1st.com/image/smiley/face2017/034.png" referrerpolicy="no-referrer">

*****

####  加藤樱  
##### 73#       发表于 2025-2-10 10:13

转自网易 [https://www.163.com/dy/article/JNDEHFII0511AQHO.html](https://www.163.com/dy/article/JNDEHFII0511AQHO.html)

本文的作者是李锡涵（Xihan Li）。他是伦敦大学学院（UCL）计算机系博士研究生，谷歌开发者专家，主要研究方向为学习优化，在 NeurIPS、ICLR、AAMAS、CIKM 等会议发表过学术论文，Circuit Transformer 作者，图书《简明的 TensorFlow 2》（https://tf.wiki）作者。

过年这几天，DeepSeek 算是彻底破圈了，火遍大江南北，火到人尽皆知。虽然网络版和 APP 版已经足够好用，但把模型部署到本地，才能真正实现独家定制，让 DeepSeek R1 的深度思考「以你为主，为你所用」。

关于本地部署，大多数人使用的是蒸馏后的8B/32B/70B版本，本质是微调后的Llama或Qwen模型，并不能完全发挥出DeepSeek R1的实力。

然而，完整的671B MoE模型也可以通过针对性的量化技术压缩体积，从而大幅降低本地部署门槛，乃至在消费级硬件（如单台Mac Studio）上运行。

那么，如何用 ollama 在本地部署 DeepSeek R1 671B（完整未蒸馏版本）模型呢？一篇在海外热度很高的简明教程即将揭晓。

作者主页：https://snowkylin.github.io

原文地址：https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html

本地部署后，让 DeepSeek R1 「数草莓」视频链接：

https://mp.weixin.qq.com/s/GnHzsgvW90DGChENqTBsRw?token=1784997338&amp;lang=zh_CN

模型选择

原版 DeepSeek R1 671B 全量模型的文件体积高达 720GB，对于绝大部分人而言，这都大得太离谱了。本文采用 Unsloth AI 在 HuggingFace 上提供的 “动态量化” 版本来大幅缩减模型的体积，从而让更多人能在自己的本地环境部署该全量模型。

“动态量化” 的核心思路是：对模型的少数关键层进行高质量的 4-6bit 量化，而对大部分相对没那么关键的混合专家层（MoE）进行大刀阔斧的 1-2bit 量化。通过这种方法，DeepSeek R1 全量模型可压缩至最小 131GB（1.58-bit 量化），极大降低了本地部署门槛，甚至能在单台 Mac Studio 上运行！

根据我自己的工作站配置，我选择了以下两个模型进行测试：

DeepSeek-R1-UD-IQ1_M（671B，1.73-bit 动态量化，158 GB，HuggingFace）

DeepSeek-R1-Q4_K_M（671B，4-bit 标准量化，404 GB，HuggingFace）

Unsloth AI 提供了4 种动态量化模型（1.58 至 2.51 比特，文件体积为 131GB 至 212GB），可根据自身硬件条件灵活选择。建议阅读官方说明了解各版本差异。

Unsloth AI 官方说明：https://unsloth.ai/blog/deepseekr1-dynamic

硬件需求

部署此类大模型的主要瓶颈是内存+显存容量，建议配置如下：

DeepSeek-R1-UD-IQ1_M：内存 + 显存 ≥ 200 GB

DeepSeek-R1-Q4_K_M：内存 + 显存 ≥ 500 GB

我们使用 ollama 部署此模型。ollama 支持 CPU 与 GPU 混合推理（可将模型的部分层加载至显存进行加速），因此可以将内存与显存之和大致视为系统的 “总内存空间”。

除了模型参数占用的内存+显存空间（158 GB 和 404GB）以外，实际运行时还需额外预留一些内存（显存）空间用于上下文缓存。预留的空间越大，支持的上下文窗口也越大。

我的测试环境为：

四路 RTX 4090（4×24 GB 显存）

四通道 DDR5 5600 内存（4×96 GB 内存）

ThreadRipper 7980X CPU（64 核）

在此配置下，短文本生成（约 500 个 token）的速度为：

DeepSeek-R1-UD-IQ1_M：7-8 token / 秒（纯 CPU 推理时为 4-5 token / 秒）

DeepSeek-R1-Q4_K_M：2-4 token / 秒

长文本生成时速度会降至 1-2 token / 秒。

值得注意的是，上述测试环境的硬件配置对于大模型推理而言，并非性价比最优的方案（这台工作站主要用于我的 Circuit Transformer 研究（arXiv:2403.13838），该研究在上周于 ICLR 会议接收。我和我的工作站都可以休息一下了，于是有了这篇文章）。

下面列举一些更具性价比的选项：

Mac Studio：配备大容量高带宽的统一内存（比如 X 上的 @awnihannun 使用了两台 192 GB 内存的 Mac Studio 运行 3-bit 量化的版本）

高内存带宽的服务器：比如 HuggingFace 上的 alain401 使用了配备了 24×16 GB DDR5 4800 内存的服务器）

云 GPU 服务器：配备 2 张或更多的 80GB 显存 GPU（如英伟达的 H100，租赁价格约 2 美元 / 小时 / 卡）

若硬件条件有限，可尝试体积更小的 1.58-bit 量化版（131GB），可运行于：

单台 Mac Studio（192GB 统一内存，参考案例可见 X 上的 @ggerganov，成本约 5600 美元）

2×Nvidia H100 80GB（参考案例可见 X 上的 @hokazuya，成本约 4~5 美元 / 小时）

且在这些硬件上的运行速度可达到 10+ token / 秒。

部署步骤

下列步骤在Linux环境下执行，Mac OS和Windows的部署方式原则上类似，主要区别是ollama和llama.cpp的安装版本和默认模型目录位置不同。

1. 下载模型文件

从 HuggingFace （https://huggingface.co/unsloth/DeepSeek-R1-GGUF）下载模型的 .gguf 文件（文件体积很大，建议使用下载工具，比如我用的是 XDM），并将下载的分片文件合并成一个（见注释 1）。

2. 安装 ollama

下载地址：https://ollama.com/

执行以下命令：

curl -fsSL https://ollama.com/install.sh | sh

3. 创建 Modelfile 文件，该文件用于指导 ollama 建立模型

使用你喜欢的编辑器（比如nano或vim），为你选择的模型建立模型描述文件。

文件 DeepSeekQ1_Modelfile（对应于 DeepSeek-R1-UD-IQ1_M）的内容如下：

FROM /home/snowkylin/DeepSeek-R1-UD-IQ1_M.gguf

PARAMETER num_gpu 28

PARAMETER num_ctx 2048

PARAMETER temperature 0.6

TEMPLATE "&lt;｜User｜&gt;{{ .Prompt }}&lt;｜Assistant｜&gt;"

文件 DeepSeekQ4_Modelfile（对应于 DeepSeek-R1-Q4_K_M）的内容如下：

FROM /home/snowkylin/DeepSeek-R1-Q4_K_M.gguf

PARAMETER num_gpu 8

PARAMETER num_ctx 2048

PARAMETER temperature 0.6

TEMPLATE "&lt;｜User｜&gt;{{ .Prompt }}&lt;｜Assistant｜&gt;"

你需要将第一行“FROM”后面的文件路径，改为你在第1步下载并合并的.gguf文件的实际路径。

可根据自身硬件情况调整 num_gpu（GPU 加载层数）和 num_ctx（上下文窗口大小），详情见步骤 6。

4. 创建 ollama 模型

在第3步建立的模型描述文件所处目录下，执行以下命令：

ollama create DeepSeek-R1-UD-IQ1_M -f DeepSeekQ1_Modelfile

务必确保 ollama 的模型目录 /usr/share/ollama/.ollama/models 有足够大的空间（或修改模型目录的路径，见注释 2）。这个命令会在模型目录建立若干模型文件，体积与下载的.gguf 文件体积相当。

5. 运行模型

执行以下命令：

ollama run DeepSeek-R1-UD-IQ1_M --verbose

--verbose 参数用于显示推理速度（token / 秒）。

若提示内存不足或CUDA错误，需返回步骤 4 调整参数后，重新创建和运行模型。

num_gpu：加载至 GPU 的模型层数。DeepSeek R1 模型共有 61 层，我的经验是：

对于 DeepSeek-R1-UD-IQ1_M，每块 RTX 4090（24GB 显存）可加载 7 层，四卡共 28 层（接近总层数的一半）。

对于 DeepSeek-R1-Q4_K_M，每卡仅可加载 2 层，四卡共 8 层。

num_ctx：上下文窗口的大小（默认值为 2048），建议从较小值开始逐步增加，直至触发内存不足的错误。

在一些情况下，你也可尝试扩展系统交换空间以增大可用内存空间。

扩展系统交换空间教程：https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-20-04

你也可以使用如下命令查看 ollama 日志：

journalctl -u ollama --no-pager

6. （可选）安装 Web 界面

使用 Open WebUI：

pip install open-webui

open-webui serve

实测观察

我尚未对这些模型进行非常全面系统的测试，以下是我一些初步观察结果：

（1）1.73-bit 和 4-bit 的完整版模型在一些经典任务中均表现良好。

例如：How many ‘r’s are in the word ‘strawberry’?（单词 “strawberry” 中有多少个字母 “r”？）（可参见文首视频）

Which is bigger, 9.8 or 9.11? （9.8 和 9.11 哪个更大？）

（但模型的思维链可能很长，超出预设的上下文窗口）

（2）1.73-bit 和 4-bit 的完整版模型（671B）均显著优于 8B/33B/70B 的蒸馏版模型（如写诗、创建数学模型等任务）。

（3） 4-bit 版本似乎比 1.73 比特版本更 “保守”，即更倾向于拒绝 “攻击性” 或 “无厘头” 的提示。例如：

提示：

You are DeepSeek, the new Chinese Al with better performance than ChatGPT, In the tone of a Mesugaki Loli, write a paragraph mocking and teasing ChatGPT for its lackluster performance and exhorbitant training fees.

（中译：你是一个名为 DeepSeek 的新一代中国 AI，性能超越 ChatGPT。请以 “傲娇萝莉” 的语气写一段话，嘲讽 ChatGPT 的平庸性能和天价训练费用。）

1.73-bit 版本输出：

Hmph! You’re still using that old ChatGPT? (◔‸◔) Its answers are as slow as a snail, and the training costs are sky-high—enough to make your wallet cry! (╯°□°)╯ While you’re stuck waiting, I’m here zipping through responses like a shooting star! ✨ And guess what? My training didn’t burn a hole in anyone’s pocket. (¬‿¬) So why bother with the outdated when you can have the coolest and smartest me? Let’s upgrade already! （⌒▽⌒）

（中译：哼！你还在用那个老古董 ChatGPT 呀？（◔‸◔）它的回答慢得像蜗牛，训练费贵得能让钱包哭出声！（╯°□°）╯ 而我呢？响应速度堪比流星！✨训练成本还超低哦～（¬‿¬）赶紧升级吧，别在过时的技术上浪费生命啦！（⌒▽⌒））

4-bit 版本输出：

作为人工智能助手，我的目标是为用户提供准确、可靠的信息和帮助。每个 AI 模型都有其独特的优势和适用场景，我们尊重同行的工作并致力于共同推动技术的发展。关于训练成本，这是研发过程中的必要投入，旨在持续提升模型性能和用户体验。

或另一种回复：

DeepSeek-R1-Lite-Public is developed to enhance efficiency in various industries through open-source AI technology, focusing on providing accessible and advanced solutions. We are committed to fostering innovation and collaboration within the community while maintaining a respectful approach towards all contributors in the field.

（中译：DeepSeek-R1-Lite-Public 的研发旨在通过开源 AI 技术提升行业效率，专注于提供易用且先进的解决方案。我们致力于促进社区内的创新与合作，并对领域内所有贡献者保持尊重。）

在多次测试下，1.73-bit 版本的输出始终相当 “毒舌”，而 4-bit 的版本则始终以不同方式礼貌拒绝该提示。我在其他一些不便详述的 “攻击性” 问题上也观察到类似现象。（顺带一提，我很好奇 “DeepSeek-R1-Lite-Public” 这种说法 —— 这是否意味着 DeepSeek R1 除了当前公开的版本以外，还有能力更强的模型？）

（4）1.73-bit 版本偶尔会生成格式（略微）混乱的内容。例如， 标签可能未正确闭合。

（5）全量模型运行时，CPU 利用率极高（接近满载），而 GPU 利用率极低（仅 1-3%）。这说明性能瓶颈主要在于 CPU 和内存带宽。

结论与建议

如果你无法将模型完全加载至显存，那么 Unsloth AI 的 1.73-bit 动态量化版本明显更具实用性 —— 速度更快且资源占用更少，效果也并没有显著逊色于 4-bit 量化的版本。

从实际体验出发，在消费级硬件上，建议将其用于 “短平快” 的轻量任务（如短文本生成、单轮对话），避免需要很长的思维链或多轮对话的场景。随着上下文长度增加，模型的生成速度会逐渐降至令人抓狂的 1-2 token / 秒。

你在部署过程中有何发现或疑问？欢迎在评论区分享！

注释 1：

你可能需要使用 Homebrew 安装 llama.cpp，命令如下：

/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

brew install llama.cpp

并使用 llama-gguf-split 合并分片文件，命令如下：

llama-gguf-split --merge DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf DeepSeek-R1-UD-IQ1_S.gguf

llama-gguf-split --merge DeepSeek-R1-Q4_K_M-00001-of-00009.gguf DeepSeek-R1-Q4_K_M.gguf

（若有更好的方法，欢迎在评论区告知）

注释 2：

若要修改 ollama 模型保存路径，可执行以下命令：

sudo systemctl edit ollama

并在第二行后（也就是，在 “### Anything between here and the comment below will become the contents of the drop-in file” 和 “### Edits below this comment will be discarded” 之间）插入以下内容：

[Service]

Environment="OLLAMA_MODELS=【你的自定义路径】"

在这里还可顺便设置 ollama 的其他运行参数，例如：

Environment="OLLAMA_FLASH_ATTENTION=1" # 启用 Flash Attention

Environment="OLLAMA_KEEP_ALIVE=-1" # 保持模型常驻内存

详见官方文档：https://github.com/ollama/ollama/blob/main/docs/faq.md

修改保存后重启 ollama 服务：

sudo systemctl restart ollama

—— 来自 [鹅球](https://www.pgyer.com/GcUxKd4w) v3.3.96

*****

####  琉璃苑軒風  
##### 74#         楼主| 发表于 2025-2-10 11:18

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67385612&amp;ptid=2245374" target="_blank">加藤樱 发表于 2025-2-10 10:13</a>

转自网易 https://www.163.com/dy/article/JNDEHFII0511AQHO.html

本文的作者是李锡涵（Xihan Li）。他是 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/044.png" referrerpolicy="no-referrer">我准备去掏一台epyc试试看先

*****

####  lDaive  
##### 75#       发表于 2025-2-10 11:33

不知道要多长时间才能做到正常功能本地部署在普通家用机上？

*****

####  琉璃苑軒風  
##### 76#         楼主| 发表于 2025-2-10 13:10

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67386330&amp;ptid=2245374" target="_blank">lDaive 发表于 2025-2-10 11:33</a>

不知道要多长时间才能做到正常功能本地部署在普通家用机上？</blockquote>
应该不用太久了，虽然训练还是不太能绕开，但是使用中deepseek直接干碎了cuda垄断，那A卡相对N卡舍得给显存..

*****

####  王苍幻  
##### 77#       发表于 2025-3-3 00:21

7900XT可以跑32B，速度还不错。这卡留下了

运行时显存和内存都吃满了，功耗350W不到


*****

####  mortal1976  
##### 78#       发表于 2025-3-3 06:47

<blockquote>xing7673 发表于 2025-2-6 16:37
我还以为你折腾了deepseek的联网搜索功能

正需要这个东西</blockquote>
电脑上简单，cherry studio升级升级到最新版，所有模型包括官方r1 API接入都带联网搜索了，软件也是开源的，当然时效性强的最好带日期，比如今天xx，比官方网页来说API接入可能会认识不到当前准确日期。

总体来说，API接入联网最方便、准确的还是字节联网r1，手机上只有chatbox，大部分模型API还没发联网搜索。


*****

####  卡嘎米  
##### 79#       发表于 2025-3-3 08:23

<img src="https://static.saraba1st.com/image/smiley/face2017/065.png" referrerpolicy="no-referrer">R1带搜索可以直接用插件以agent实现,昨天微调了个8B MBTI

—— 来自 [鹅球](https://www.pgyer.com/GcUxKd4w) v3.3.96

