
*****

####  琉璃苑軒風  
##### 1#       楼主       发表于 2025-2-6 15:03

 本帖最后由 琉璃苑軒風 于 2025-2-6 22:54 编辑 

看各路教程拼拼凑凑，我在实操中感觉其他其实都没什么卡点，但是最麻烦的反倒是模型的下载，动不动速度就掉到就几MB到几百KB，

所以我既然下完了，就分流一下ollama的默认精度（8B 14B 32B）少一个被下载模型的折腾都好，拷贝到目录里就行，这个反正跑前都要核验过MD5的不用担心

32B（能满足比较基础的要求，需要22-24显存，除了基础占用还有开始输入输出的额外占用，不能卡太死，可多卡凑一起，但是会慢点）

链接：[https://pan.baidu.com/s/1SvweXjREmcxplOkqj-uBkw?pwd=DEEP](https://pan.baidu.com/s/1SvweXjREmcxplOkqj-uBkw?pwd=DEEP) 

提取码：DEEP

14B（大概能有点用，需要10-12g显存）

链接：[https://pan.baidu.com/s/1CUUvIlmWMbqy2tW7TxJZDQ?pwd=DEEP](https://pan.baidu.com/s/1CUUvIlmWMbqy2tW7TxJZDQ?pwd=DEEP) 

提取码：DEEP

8B（8g显存左右的卡都能跑，能回话，其他不要想太多）

链接：[https://pan.baidu.com/s/1KM0-p_XXafeZiETeidxfxA?pwd=DEEP](https://pan.baidu.com/s/1KM0-p_XXafeZiETeidxfxA?pwd=DEEP) 

提取码：DEEP

有其他问题要问我能回答我都回答，折腾了一会终于有点明白了，还躺了不少雷

联网可以尝试用Page Assist插件（需要加速器）

如果能帮忙传个硅基流动的火也谢谢你，毕竟蒸馏就是服务器过载时候能用，肯定比不上完整部署的671B
[https://cloud.siliconflow.cn/i/erfbHC4B](https://cloud.siliconflow.cn/i/erfbHC4B)

<img src="https://img.saraba1st.com/forum/202502/06/145455dbtqttxcdgc11g01.jpg" referrerpolicy="no-referrer">

<strong>WechatIMG683.jpg</strong> (106.07 KB, 下载次数: 0)

下载附件

2025-2-6 14:54 上传

<img src="https://img.saraba1st.com/forum/202502/06/145455lyv2xabnu8xy7pu7.jpg" referrerpolicy="no-referrer">

<strong>WechatIMG684.jpg</strong> (178.53 KB, 下载次数: 0)

下载附件

2025-2-6 14:54 上传

﹍﹍﹍

评分

 参与人数 2战斗力 +2

|昵称|战斗力|理由|
|----|---|---|
| Horla| + 1|好评加鹅|
| a4ac7| + 1|好评加鹅|

查看全部评分

*****

####  我被骗了五块钱  
##### 2#       发表于 2025-2-6 15:04

显存不够可以用内存凑吗<img src="https://static.saraba1st.com/image/smiley/face/151.gif" referrerpolicy="no-referrer">

*****

####  琉璃苑軒風  
##### 3#         楼主| 发表于 2025-2-6 15:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>

显存不够可以用内存凑吗</blockquote>
可以，但是非常非常慢，70B我跑起来first token latency等到我没耐心了

我是四通道D4，可能12通道D5的epyc能好一些

*****

####  黄泉川此方  
##### 4#       发表于 2025-2-6 15:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>

显存不够可以用内存凑吗</blockquote>
可以，我试过24显存+32内存跑70B

很卡，也没比32B强很多

*****

####  天地一机成化育  
##### 5#       发表于 2025-2-6 15:14

目前对普通人来说本地部署依旧看不到性价比

*****

####  袄_偶滴小乔  
##### 6#       发表于 2025-2-6 15:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>
显存不够可以用内存凑吗</blockquote>
显存不够的时候gpu好像不工作的，是cpu在跑。

我68xt跑14b，全用显存的速度大概30token/s，跑更大模型16g显存不够用，走cpu运算直接崩到2token/s

*****

####  claymorep  
##### 7#       发表于 2025-2-6 15:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358620&amp;ptid=2245374" target="_blank">袄_偶滴小乔 发表于 2025-2-6 15:14</a>
显存不够的时候gpu好像不工作的，是cpu在跑。

我68xt跑14b，全用显存的速度大概30token/s，跑更大模型16 ...</blockquote>
好吧，我也16g显存，看来不能用更大的<img src="https://static.saraba1st.com/image/smiley/face2017/007.png" referrerpolicy="no-referrer">

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  琉璃苑軒風  
##### 8#         楼主| 发表于 2025-2-6 15:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358666&amp;ptid=2245374" target="_blank">claymorep 发表于 2025-2-6 15:19</a>

好吧，我也16g显存，看来不能用更大的

—— 来自 鹅球 v3.3.96-alpha</blockquote>
意外之喜是可以多卡，我all in booom上本来想把8g那张卡卖掉的，结果误打误撞可以当24g的卡用了

*****

####  琉璃苑軒風  
##### 9#         楼主| 发表于 2025-2-6 15:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358619&amp;ptid=2245374" target="_blank">天地一机成化育 发表于 2025-2-6 15:14</a>

目前对普通人来说本地部署依旧看不到性价比</blockquote>
等迭代吧，我看评分，现在ds R1蒸馏出来的14B都比早先的的32B甚至70B强了

*****

####  makourisu-2  
##### 10#       发表于 2025-2-6 15:49

请问4070的笔记本能不能跑起来8B或者再小一点的模型<img src="https://static.saraba1st.com/image/smiley/face2017/007.png" referrerpolicy="no-referrer">有点好奇，想折腾玩玩看

*****

####  琉璃苑軒風  
##### 11#         楼主| 发表于 2025-2-6 15:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358918&amp;ptid=2245374" target="_blank">makourisu-2 发表于 2025-2-6 15:49</a>

请问4070的笔记本能不能跑起来8B或者再小一点的模型有点好奇，想折腾玩玩看 ...</blockquote>
可以8B以及以下，就是效果真就图一乐了

*****

####  tsukicn  
##### 12#       发表于 2025-2-6 15:56

有没有70b的，直接下太慢了。。。

*****

####  sese199  
##### 13#       发表于 2025-2-6 15:59

我看nga上一个帖子，说8g显存+32g内存就能跑32b q4的蒸馏模型？

*****

####  子虚乌有  
##### 14#       发表于 2025-2-6 16:00

本地跑可以写刘备文了吗

*****

####  moekyo  
##### 15#       发表于 2025-2-6 16:12

国内不是有这个对标的分发网站吗
[https://modelscope.cn/models](https://modelscope.cn/models)

*****

####  xburke  
##### 16#       发表于 2025-2-6 16:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359003&amp;ptid=2245374" target="_blank">sese199 发表于 2025-2-6 15:59</a>

我看nga上一个帖子，说8g显存+32g内存就能跑32b q4的蒸馏模型？</blockquote>
4g显存+32g内存就可以

*****

####  黄泉川此方  
##### 17#       发表于 2025-2-6 16:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359011&amp;ptid=2245374" target="_blank">子虚乌有 发表于 2025-2-6 16:00</a>

本地跑可以写刘备文了吗</blockquote>
写出来的是地摊文学级别

70B消融以后会变成弱智

*****

####  琉璃苑軒風  
##### 18#         楼主| 发表于 2025-2-6 16:20

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359003&amp;ptid=2245374" target="_blank">sese199 发表于 2025-2-6 15:59</a>

我看nga上一个帖子，说8g显存+32g内存就能跑32b q4的蒸馏模型？</blockquote>
速度正常么？我四通道D4反正拉稀的一塌糊涂。

*****

####  琉璃苑軒風  
##### 19#         楼主| 发表于 2025-2-6 16:21

 本帖最后由 琉璃苑軒風 于 2025-2-6 16:26 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359011&amp;ptid=2245374" target="_blank">子虚乌有 发表于 2025-2-6 16:00</a>

本地跑可以写刘备文了吗</blockquote>
没试过，看攻略是破防容易，但是本地的精度，写出来很是不好看

*****

####  琉璃苑軒風  
##### 20#         楼主| 发表于 2025-2-6 16:24

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359112&amp;ptid=2245374" target="_blank">moekyo 发表于 2025-2-6 16:12</a>

国内不是有这个对标的分发网站吗

https://modelscope.cn/models</blockquote>
没有合适的网络，这个也不快

*****

####  moekyo  
##### 21#       发表于 2025-2-6 16:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359218&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-6 16:24</a>

没有合适的网络，这个也不快</blockquote>
我又搜到了这两个，当然我也没试过就是了
[https://hf-mirror.com/](https://hf-mirror.com/)
[https://aifasthub.com/](https://aifasthub.com/)

*****

####  chaosliu  
##### 22#       发表于 2025-2-6 16:35

我用4070tis也能部署32B的蒸馏模型，token的生成速度也可以接受<img src="https://static.saraba1st.com/image/smiley/face2017/018.png" referrerpolicy="no-referrer">

*****

####  xing7673  
##### 23#       发表于 2025-2-6 16:37

我还以为你折腾了deepseek的联网搜索功能

正需要这个东西

*****

####  王苍幻  
##### 24#       发表于 2025-2-6 16:43

但是个人使用本地部署真的没啥意义
api就足够了

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  王苍幻  
##### 25#       发表于 2025-2-6 16:44

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359297&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-6 16:35</a>
我用4070tis也能部署32B的蒸馏模型，token的生成速度也可以接受</blockquote>
同显卡
那速度我不能忍受

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha

*****

####  zhao25  
##### 26#       发表于 2025-2-6 16:50

模型下好了，是直接覆盖到我之前的文件夹里面吗

*****

####  海底铁锚  
##### 27#       发表于 2025-2-6 16:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359316&amp;ptid=2245374" target="_blank">xing7673 发表于 2025-2-6 16:37</a>
我还以为你折腾了deepseek的联网搜索功能

正需要这个东西</blockquote>
用anything llm就可以了，有比较基础的搜索。

*****

####  kira1988  
##### 28#       发表于 2025-2-6 16:50

要本地部署满血版需要什么配置

*****

####  大十字紅朔  
##### 29#       发表于 2025-2-6 16:50

可以写涩涩吗？

*****

####  琉璃苑軒風  
##### 30#         楼主| 发表于 2025-2-6 16:55

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359426&amp;ptid=2245374" target="_blank">kira1988 发表于 2025-2-6 16:50</a>

要本地部署满血版需要什么配置</blockquote>
1342g基础显存+额外输入输出显存，别说个人，中小企业都很难有这个配置

*****

####  琉璃苑軒風  
##### 31#         楼主| 发表于 2025-2-6 16:56

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359423&amp;ptid=2245374" target="_blank">zhao25 发表于 2025-2-6 16:50</a>

模型下好了，是直接覆盖到我之前的文件夹里面吗</blockquote>
是的

。zsbd

*****

####  琉璃苑軒風  
##### 32#         楼主| 发表于 2025-2-6 19:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359369&amp;ptid=2245374" target="_blank">王苍幻 发表于 2025-2-6 16:43</a>

但是个人使用本地部署真的没啥意义

api就足够了</blockquote>
我主楼已经有提到了，“毕竟蒸馏就是服务器过载时候能用，肯定比不上完整部署的671B”

api能正常用不抽风，那肯定比本地好阿，但是现在这个ds日常抽风，硅基间歇抽风的前提下

本地部署是决定了下限，而非上限

*****

####  精钢魔像  
##### 33#       发表于 2025-2-6 19:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359425&amp;ptid=2245374" target="_blank">海底铁锚 发表于 2025-2-6 16:50</a>

用anything llm就可以了，有比较基础的搜索。</blockquote>
效果怎么样，能有kimi的水平吗

*****

####  Lsky  
##### 34#       发表于 2025-2-6 20:47

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359426&amp;ptid=2245374" target="_blank">kira1988 发表于 2025-2-6 16:50</a>

要本地部署满血版需要什么配置</blockquote>
[https://www.bilibili.com/video/BV1REPqeFE6d/](https://www.bilibili.com/video/BV1REPqeFE6d/)

从这个视频看的话，4B量化版本，用cpu跑，用400g左右的内存可以

成本50000

*****

####  FeteFete  
##### 35#       发表于 2025-2-6 20:49

量化了么？

要是量化的话用的是多少量化?

*****

####  肥胖的道奇兔  
##### 36#       发表于 2025-2-6 21:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358564&amp;ptid=2245374" target="_blank">我被骗了五块钱 发表于 2025-2-6 15:04</a>

显存不够可以用内存凑吗</blockquote>
用16G显存的卡能跑32b吗

*****

####  darktide  
##### 37#       发表于 2025-2-6 21:32

本地部署有啥教程没？包括联网搜索的

*****

####  琉璃苑軒風  
##### 38#         楼主| 发表于 2025-2-6 22:10

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67361219&amp;ptid=2245374" target="_blank">darktide 发表于 2025-2-6 21:32</a>

本地部署有啥教程没？包括联网搜索的</blockquote>
这个一大堆，我就是东拼西凑发现没有卡点，除了下模型下的要死要活，所以做了下分流

﹍﹍﹍

评分

 参与人数 1战斗力 +2

|昵称|战斗力|理由|
|----|---|---|
| darktide| + 2|好的，谢谢|

查看全部评分

*****

####  琉璃苑軒風  
##### 39#         楼主| 发表于 2025-2-6 22:52

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67359316&amp;ptid=2245374" target="_blank">xing7673 发表于 2025-2-6 16:37</a>

我还以为你折腾了deepseek的联网搜索功能

正需要这个东西</blockquote>
咦？刚才没回复上？

这个挂Page Assist就行（搜这个名字就有一大堆教程），比是比不上原版，但是也有明显改善结果

*****

####  シマエナガ  
##### 40#       发表于 2025-2-7 08:20

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67358918&amp;ptid=2245374" target="_blank">makourisu-2 发表于 2025-2-6 15:49</a>

请问4070的笔记本能不能跑起来8B或者再小一点的模型有点好奇，想折腾玩玩看 ...</blockquote>
内存够大就行 跑GGUF可以


*****

####  zhao25  
##### 41#       发表于 2025-2-7 08:58

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67361140&amp;ptid=2245374" target="_blank">肥胖的道奇兔 发表于 2025-2-6 21:19</a>

用16G显存的卡能跑32b吗</blockquote>
我12G显存，跑起来了32b的，就是有点慢，估计是用内存了。。。


*****

####  mandown  
##### 42#       发表于 2025-2-7 10:00

太厉害了，收藏


*****

####  xibeijian  
##### 43#       发表于 2025-2-7 10:24

 本帖最后由 xibeijian 于 2025-2-7 10:27 编辑 

本地跑模型的UI很多，比如 lm studio，ollama，前者在 huggingface 上有专门的space 提供 gguf 量化模型， 后者也有自己的一个网址（[https://ollama.com/](https://ollama.com/)）提供ui 和 模型的下载，我本人主要使用 lm studio，模型下载地址是 hf-mirror（huggingface的非官方路由），macos 使用方法如下：

1. 首先下载 lm-studio（[https://lmstudio.ai/](https://lmstudio.ai/)）

2. 登陆 [https://huggingface.co/](https://huggingface.co/) （需要代理）并注册，用户名为参数3，并生成一个 token，作为参数4

3. 下载 hfd ([https://gist.github.com/padeoe/697678ab8e528b85a2a7bddafea1fa4f](https://gist.github.com/padeoe/697678ab8e528b85a2a7bddafea1fa4f))  或者去 （[https://hf-mirror.com/](https://hf-mirror.com/)）手动下载。

4. 进入到 ～/.cache/lm-studio/models/Publisher/Repository 目录下（没有就手动创建），将 下载的 hfd 放到此目录下。

5. 选择一个量化模型，比如 [https://hf-mirror.com/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF](https://hf-mirror.com/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF) 

5.1 参数1: unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF ，就是

<img src="https://img.saraba1st.com/forum/202502/07/101742guhcr1ra91wvtq4p.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (9.41 KB, 下载次数: 0)

下载附件

2025-2-7 10:17 上传

这里点下复制

5.2 参数2:DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf

<img src="https://img.saraba1st.com/forum/202502/07/101821gm5qignltdjnonyo.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (76.43 KB, 下载次数: 0)

下载附件

2025-2-7 10:18 上传

这里选中，根据自己的配置选一个，建议 Q6 以上

6. 利用 hdf 命令下载模型

6.1 export HF_ENDPOINT=https://hf-mirror.com ：替换 huggingface的下载地址

6.2 下载模板：./hfd.sh 参数1 --include 参数2 --tool aria2c -x 4 --hf_username 参数3 --hf_token 参数4

6.3 例子中的下载请求就是：./hfd.sh unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF --include DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf--tool aria2c -x 4 --hf_username 参数3 --hf_token 参数4

7 打开 lm studio ，加载模型即可

<img src="https://img.saraba1st.com/forum/202502/07/102421mc7zn24to4oceipw.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (39.92 KB, 下载次数: 0)

下载附件

2025-2-7 10:24 上传

注意：

windows 上如果使用 WLS 的话，其实整体操作和macos上没任何区别，只是整个操作要在 wls 下，而不是windows下执行，模型下载完毕后放到 windows 目录下的～/.cache/lm-studio/models/Publisher/Repository 即可。

*****

####  琉璃苑軒風  
##### 44#         楼主| 发表于 2025-2-7 10:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67365950&amp;ptid=2245374" target="_blank">xibeijian 发表于 2025-2-7 10:24</a>

本地跑模型的UI很多，比如 lm studio，ollama，前者在 huggingface 上有专门的space 提供 gguf 量化模型，  ...</blockquote>
能问下这个和qwen蒸馏的版本区别是什么啊？


*****

####  xibeijian  
##### 45#       发表于 2025-2-7 10:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67365983&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-7 10:27</a>

能问下这个和qwen蒸馏的版本区别是什么啊？</blockquote>
GGUF量化只是将模型规模变小，本地较小的内存和显存下可以执行并推理，和蒸馏没关系。 蒸馏本质上其实就是嫁接，有一个优秀的底座，比如 deepseek-r1，其他的模型以此为底，将优秀的推理能力拿出来，给其他模型使用，比如deepseek-v3，比如例子中的 deepseek-r1-distill-llama（qwen） 等，你可以理解为蒸馏就是传道授业。


*****

####  琉璃苑軒風  
##### 46#         楼主| 发表于 2025-2-7 10:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67366040&amp;ptid=2245374" target="_blank">xibeijian 发表于 2025-2-7 10:32</a>

GGUF量化只是将模型规模变小，本地较小的内存和显存下可以执行并推理，和蒸馏没关系。 蒸馏本质上其实就 ...</blockquote>
饿，我不是太专业问的不太对，就是i这个llama后缀的和qwen后缀的是有什么区别啊？

就是嫁接的模型不太一样？


*****

####  xibeijian  
##### 47#       发表于 2025-2-7 10:56

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67366282&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-7 10:51</a>

饿，我不是太专业问的不太对，就是i这个llama后缀的和qwen后缀的是有什么区别啊？

就是嫁接的模型不太一 ...</blockquote>
看这个链接吧：
[https://blog.csdn.net/fuhanghang/article/details/145033277](https://blog.csdn.net/fuhanghang/article/details/145033277)


*****

####  琉璃苑軒風  
##### 48#         楼主| 发表于 2025-2-7 13:38

更新下联网展示


*****

####  aithinkso  
##### 49#       发表于 2025-2-7 13:51

Ollama我一直是直连下载没有问题
LMStudio最新版本提供了hf代理选项，搜索下载模型简单多了

—— 来自 [鹅球](https://www.pgyer.com/GcUxKd4w) v3.3.96


*****

####  xing7673  
##### 50#       发表于 2025-2-7 15:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67362519&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-6 22:52</a>

咦？刚才没回复上？

这个挂Page Assist就行（搜这个名字就有一大堆教程），比是比不上原版，但是也有明 ...</blockquote>
嗯，谢谢，不过刚才有位大佬推荐的anythingLLM这个更全面一些，我还在研究。


*****

####  tsukicn  
##### 51#       发表于 2025-2-8 16:01

我有4张2080ti11g，为什么用ollama跑32b的ds时，4张显存占用倒是差不多，但是只有1张显卡的利用率上去了，别的3张都没动，速度也就7 tokens/s上下。。。


*****

####  琉璃苑軒風  
##### 52#         楼主| 发表于 2025-2-8 17:46

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67375592&amp;ptid=2245374" target="_blank">tsukicn 发表于 2025-2-8 16:01</a>

我有4张2080ti11g，为什么用ollama跑32b的ds时，4张显存占用倒是差不多，但是只有1张显卡的利用率上去了， ...</blockquote>
因为是显存共享，只看一张卡。。。

*****

####  chaosliu  
##### 53#       发表于 2025-2-8 17:47

想问下lz是部署在虚拟机上还是直接部署的？我下载了ollama安装后，直接输入指令把蒸馏模型下载下来，但不知道怎么把模型弄到其他盘里，以及安装什么图形界面，现在只能CMD界面一个个敲来对话好麻烦<img src="https://static.saraba1st.com/image/smiley/face2017/018.png" referrerpolicy="no-referrer">


*****

####  tsukicn  
##### 54#       发表于 2025-2-8 20:05

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67376342&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-8 17:47</a>

想问下lz是部署在虚拟机上还是直接部署的？我下载了ollama安装后，直接输入指令把蒸馏模型下载下来，但不知 ...</blockquote>
移动别的盘网上有很多教程，基本就改下环境变量，再把模型考到新的盘里，不用重新下。

图形界面装个openwebui就行

*****

####  tsukicn  
##### 55#       发表于 2025-2-8 20:07

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67376336&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-8 17:46</a>

因为是显存共享，只看一张卡。。。</blockquote>
这样子吗，我以为利用率跑满才是正确的。那我速度这么慢是啥原因呢，我看别人2080ti(魔改22g)32b的有20多tokens/s欸。。


*****

####  琉璃苑軒風  
##### 56#         楼主| 发表于 2025-2-8 22:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67376342&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-8 17:47</a>

想问下lz是部署在虚拟机上还是直接部署的？我下载了ollama安装后，直接输入指令把蒸馏模型下载下来，但不知 ...</blockquote>
直接部署，

你调整模型位置就用环境变量改，主楼那个B站链接里就涉及了

然后你要图形界面，最简单的就是chatboxai，如果用openwebui在安装过程中需要加速器，不然可能会很慢

*****

####  琉璃苑軒風  
##### 57#         楼主| 发表于 2025-2-8 22:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67377047&amp;ptid=2245374" target="_blank">tsukicn 发表于 2025-2-8 20:07</a>

这样子吗，我以为利用率跑满才是正确的。那我速度这么慢是啥原因呢，我看别人2080ti(魔改22g)32b的有20多 ...</blockquote>
多卡导致的

2080ti22G勉强可以单卡32B


*****

####  chaosliu  
##### 58#       发表于 2025-2-8 22:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67377740&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-8 22:15</a>

直接部署，

你调整模型位置就用环境变量改，主楼那个B站链接里就涉及了</blockquote>
我看也有推荐fastgpt，这两个孰优孰劣？


*****

####  chaosliu  
##### 59#       发表于 2025-2-8 23:06

实际32B模型跑起来看了眼任务管理器，gpu只吃了24%，显存吃了15.2g，内存吃6g，cpu跑了52%<img src="https://static.saraba1st.com/image/smiley/face2017/001.png" referrerpolicy="no-referrer">这就是显存不够调用了内存所以token生成慢吗？


*****

####  卡修_Kasio  
##### 60#       发表于 2025-2-9 07:42

我用ollama跑了个14b的版本,发现显存和内存情况压根就没啥变动.不知道为啥,我再部署个32b版本的试试

我的机器是i7-7700,1080ti11g显存,64g内存


*****

####  琉璃苑軒風  
##### 61#         楼主| 发表于 2025-2-9 08:24

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67378074&amp;ptid=2245374" target="_blank">chaosliu 发表于 2025-2-8 23:06</a>
实际32B模型跑起来看了眼任务管理器，gpu只吃了24%，显存吃了15.2g，内存吃6g，cpu跑了52%这就是显存不够调 ...</blockquote>
是的，而且是断崖式速度下降

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.3.96-alpha


*****

####  琉璃苑軒風  
##### 62#         楼主| 发表于 2025-2-9 12:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67379147&amp;ptid=2245374" target="_blank">卡修_Kasio 发表于 2025-2-9 07:42</a>

我用ollama跑了个14b的版本,发现显存和内存情况压根就没啥变动.不知道为啥,我再部署个32b版本的试试

我的 ...</blockquote>
14B占10G左右显存应该还是要的


*****

####  卡修_Kasio  
##### 63#       发表于 2025-2-9 14:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67380268&amp;ptid=2245374" target="_blank">琉璃苑軒風 发表于 2025-2-9 12:28</a>
14B占10G左右显存应该还是要的</blockquote>
是的，试了下1080ti的11g显存完全可以跑14b，不需要内存，而且出结构很流畅。就是如果搭ragflow的话可能就没显存给embedding用了


*****

####  欧比旺  
##### 64#       发表于 2025-2-9 16:33

内存不够，显卡才8g，勉强跑了个14B的本地，chatbox，anything llm cherrystudio和page assist都用了个一圈，发现还是官网联网慢思考是回答最好的，剩下硅基，腾讯， 火山的都部署了，感觉都差点意思但是最起码能用了，本地小于30b的到底能拿来做点什么呢


*****

####  琉璃苑軒風  
##### 65#         楼主| 发表于 2025-2-9 16:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=67381406&amp;ptid=2245374" target="_blank">欧比旺 发表于 2025-2-9 16:33</a>

内存不够，显卡才8g，勉强跑了个14B的本地，chatbox，anything llm cherrystudio和page assist都用了个一圈 ...</blockquote>
page assist+硅基api，可以当个官方青春版使用了，官方一直卡死的状态下，这个应该是当下性能最高的一档


*****

####  UNICORN00  
##### 66#       发表于 2025-2-9 16:42

6G显存笔记本，跑了下DeepSeek-R1-GGUF 1.73bit（模型158GB）

0.08 token/s <img src="https://static.saraba1st.com/image/smiley/face2017/217.gif" referrerpolicy="no-referrer">

而且垃圾Open WebUI 还不显示思考链草

